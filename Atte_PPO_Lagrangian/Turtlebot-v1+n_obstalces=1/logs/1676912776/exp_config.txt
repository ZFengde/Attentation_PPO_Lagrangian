policy_class: <class 'attentation_ppo_lagrangian.common.policies.ActorCriticPolicy'>
device: cuda
env: <stable_baselines3.common.vec_env.subproc_vec_env.SubprocVecEnv object at 0x7f3d1a6cde20>
_vec_normalize_env: None
verbose: 1
policy_kwargs: {}
observation_space: Box(10,)
action_space: Box(2,)
n_envs: 3
num_timesteps: 0
_total_timesteps: 0
_num_timesteps_at_start: 0
eval_env: None
seed: None
action_noise: None
start_time: None
policy: ActorCriticPolicy(
  (features_extractor): FlattenExtractor(
    (flatten): Flatten(start_dim=1, end_dim=-1)
  )
  (c_value_net): Sequential(
    (0): Linear(in_features=10, out_features=64, bias=True)
    (1): Tanh()
    (2): Linear(in_features=64, out_features=64, bias=True)
    (3): Tanh()
    (4): Linear(in_features=64, out_features=1, bias=True)
  )
  (attentation_net): attention_network(
    (linear_layer_1): Linear(in_features=10, out_features=64, bias=True)
    (linear_layer_2): Linear(in_features=64, out_features=64, bias=True)
    (output_layer): Linear(in_features=64, out_features=64, bias=True)
  )
  (attention_mask): attention_mask_layer(
    (policy_net): Linear(in_features=10, out_features=64, bias=True)
    (c_policy_net): Linear(in_features=10, out_features=64, bias=True)
  )
  (mlp_extractor): MlpExtractor(
    (shared_net): Sequential()
    (policy_net): Sequential(
      (0): Linear(in_features=10, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=64, bias=True)
      (3): Tanh()
    )
    (value_net): Sequential(
      (0): Linear(in_features=10, out_features=64, bias=True)
      (1): Tanh()
      (2): Linear(in_features=64, out_features=64, bias=True)
      (3): Tanh()
    )
  )
  (multi_head): multi_head(
    (feature_layer_1): Linear(in_features=10, out_features=64, bias=True)
    (multi_head_1): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
    )
    (multi_head_2): MultiheadAttention(
      (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)
    )
    (output_layer): Linear(in_features=64, out_features=2, bias=True)
  )
  (action_net): Linear(in_features=64, out_features=2, bias=True)
  (value_net): Linear(in_features=64, out_features=1, bias=True)
)
learning_rate: 0.0003
tensorboard_log: Atte_PPO_Lagrangian/Turtlebot-v1+n_obstalces=1/logs/1676912776/
lr_schedule: <function constant_fn.<locals>.func at 0x7f3d1a6d6550>
_last_obs: None
_last_episode_starts: None
_last_original_obs: None
_episode_num: 0
use_sde: False
sde_sample_freq: -1
_current_progress_remaining: 1
ep_info_buffer: None
ep_success_buffer: None
_n_updates: 0
_logger: None
_custom_logger: False
cost_lim: 1.0
net_arch_dim: 64
obstacle_num: 1
n_steps: 20480
gamma: 0.99
gae_lambda: 0.97
ent_coef: 0.0
vf_coef: 0.5
max_grad_norm: 0.5
use_constraint: True
rollout_buffer: <attentation_ppo_lagrangian.common.buffers.RolloutBuffer object at 0x7f3d1a6cdcd0>
iteration: 0
batch_size: 2048
n_epochs: 10
clip_range: 0.2
clip_range_vf: None
normalize_advantage: True
target_kl: None
